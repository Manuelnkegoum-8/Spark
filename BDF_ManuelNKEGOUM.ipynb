{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/spark-3.2.2/spark-3.2.2-bin-hadoop2.7.tgz   \n",
        "!tar xf spark-3.2.2-bin-hadoop2.7.tgz  \n",
        "!rm spark-3.2.2-bin-hadoop2.7.tgz    \n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1lcj5PeQsUk",
        "outputId": "295f962e-3717-4a51-8ab0-2b4f822dc849"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [3 In\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [1,038 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,493 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,338 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,561 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,067 kB]\n",
            "Fetched 11.8 MB in 7s (1,757 kB/s)\n",
            "Reading package lists... Done\n",
            "--2022-11-24 09:05:48--  http://apache.osuosl.org/spark/spark-3.2.2/spark-3.2.2-bin-hadoop2.7.tgz\n",
            "Resolving apache.osuosl.org (apache.osuosl.org)... 64.50.233.100, 64.50.236.52, 140.211.166.134, ...\n",
            "Connecting to apache.osuosl.org (apache.osuosl.org)|64.50.233.100|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 272846416 (260M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.2.2-bin-hadoop2.7.tgz’\n",
            "\n",
            "spark-3.2.2-bin-had 100%[===================>] 260.21M  70.7MB/s    in 8.0s    \n",
            "\n",
            "2022-11-24 09:05:56 (32.5 MB/s) - ‘spark-3.2.2-bin-hadoop2.7.tgz’ saved [272846416/272846416]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://repos.spark-packages.org/graphframes/graphframes/0.8.1-spark3.0-s_2.12/graphframes-0.8.1-spark3.0-s_2.12.jar\n",
        "!wget https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WLVzpWCxlJF",
        "outputId": "cb138f17-6528-43bd-f6a6-d038c5f0aaa5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-24 09:06:04--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n",
            "Resolving repos.spark-packages.org (repos.spark-packages.org)... 99.84.160.25, 99.84.160.20, 99.84.160.46, ...\n",
            "Connecting to repos.spark-packages.org (repos.spark-packages.org)|99.84.160.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 247880 (242K) [binary/octet-stream]\n",
            "Saving to: ‘graphframes-0.8.2-spark3.2-s_2.12.jar’\n",
            "\n",
            "\r          graphfram   0%[                    ]       0  --.-KB/s               \rgraphframes-0.8.2-s 100%[===================>] 242.07K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-11-24 09:06:04 (7.85 MB/s) - ‘graphframes-0.8.2-spark3.2-s_2.12.jar’ saved [247880/247880]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/BDF/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/spark-3.2.2-bin-hadoop2.7 /content/spark\n",
        "\n",
        "#!mv graphframes-0.8.1-spark3.0-s_2.12.jar /content/spark/jars/\n",
        "!mv graphframes-0.8.2-spark3.2-s_2.12.jar /content/spark/jars/\n",
        "\n",
        "!export SPARK_HOME=/content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\"\n",
        "\n",
        "!ls -l /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pZUO4U6QyCu",
        "outputId": "b3f416ae-7e02-43dc-d11a-03f4fcf8e460"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/spark': No such file or directory\n",
            "/content/spark/\n",
            "DRIVE_DATA=/content/gdrive/My Drive/BDF/data/\n",
            "total 8\n",
            "drwxr-xr-x  1 root root 4096 Nov 22 00:14 sample_data\n",
            "lrwxrwxrwx  1 root root   34 Nov 24 09:06 spark -> /content/spark-3.2.2-bin-hadoop2.7\n",
            "drwxr-xr-x 13 1000 1000 4096 Jul 11 16:16 spark-3.2.2-bin-hadoop2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate() # We get the SparkContext\n",
        "sc.addPyFile('/content/spark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar')\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOO0iFoBQ4b1",
        "outputId": "1f4c1e94-b475-4313-db88-7a01f573ca13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 3 : WORD COUNT**"
      ],
      "metadata": {
        "id": "bmJSNrOgUFMi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m1Bwh5jKP5Kl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2feadf0d-f0f8-431f-ed10-5a45461aa135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------+\n",
            "|value                                                                      |\n",
            "+---------------------------------------------------------------------------+\n",
            "|The Project Gutenberg EBook of Don Quijote, by Miguel de Cervantes Saavedra|\n",
            "|                                                                           |\n",
            "|This eBook is for the use of anyone anywhere at no cost and with           |\n",
            "|almost no restrictions whatsoever.  You may copy it, give it away or       |\n",
            "|re-use it under the terms of the Project Gutenberg License included        |\n",
            "|with this eBook or online at www.gutenberg.net                             |\n",
            "|                                                                           |\n",
            "|                                                                           |\n",
            "|Title: Don Quijote                                                         |\n",
            "|                                                                           |\n",
            "|Author: Miguel de Cervantes Saavedra                                       |\n",
            "|                                                                           |\n",
            "|Posting Date: April 27, 2010 [EBook #2000]                                 |\n",
            "|Release Date: December, 1999                                               |\n",
            "|                                                                           |\n",
            "|Language: Spanish                                                          |\n",
            "|                                                                           |\n",
            "|                                                                           |\n",
            "|*** START OF THIS PROJECT GUTENBERG EBOOK DON QUIJOTE ***                  |\n",
            "|                                                                           |\n",
            "+---------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "The File has 384442 words\n"
          ]
        }
      ],
      "source": [
        "import re #Regular expressions\n",
        "dfQuijote = spark.read.text(os.environ[\"DRIVE_DATA\"] + \"/quijote.txt\")\n",
        "dfQuijote.show(truncate=False)\n",
        "def count_word(row): #count words in a string \n",
        "  result = len(re.findall(r'\\w+', row[0]))\n",
        "  return(result)\n",
        "test = dfQuijote.rdd.map(count_word)\n",
        "print(\"The File has {0} words\".format(sum(test.collect())))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 4.1 : PI**"
      ],
      "metadata": {
        "id": "7S5shf2jUNs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pyspark.sql.functions import col,expr\n",
        "import random\n",
        "N = 100000\n",
        "data = [(random.random(),random.random()) for _ in range(N)]\n",
        "df = spark.createDataFrame(data , schema =[\"X\",\"Y\"])\n",
        "df = df.withColumn(\"square\", expr(\"X*X+Y*Y\"))\n",
        "df.show(10)\n",
        "colsquare = col('square')\n",
        "Pi  = 4*df.filter(colsquare<1).count()/N\n",
        "print(\"The approximation of PI is   {0} \".format(Pi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLP2lzNyUR5R",
        "outputId": "80e1b00d-9f66-4407-afcb-6b3017641c81"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------------------+------------------+\n",
            "|                 X|                  Y|            square|\n",
            "+------------------+-------------------+------------------+\n",
            "|0.8608611704168155| 0.8017250360648747| 1.383844988184634|\n",
            "|0.9209875284849872|0.11215788526027781|0.8607974188509429|\n",
            "|0.7949338123244742| 0.5133042547905068|0.8954010239627599|\n",
            "|0.5826867561200484|0.11857216680210314|0.3535832144978505|\n",
            "|0.8816700418650529| 0.5927552296135952|1.1287008249565902|\n",
            "|0.1645531611316493| 0.7113093978141657|0.5330388022571696|\n",
            "|0.9920632170735892|0.48233813378752055|1.2168395019760274|\n",
            "| 0.650340114956127| 0.6339243330224384|0.8248023251190919|\n",
            "|0.5800414698565535| 0.6860780621691845|0.8071512141431745|\n",
            "|0.5144832181505655| 0.8627551279050695|1.0090393924850551|\n",
            "+------------------+-------------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "The approximation of PI is   3.14268 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 4.2 INSPECT LOGS**"
      ],
      "metadata": {
        "id": "LHtBYsLwdAOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "Zkn6u43BdUDY",
        "outputId": "641482bc-85c5-440b-9ee6-63e62a64fbd2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-af303a47-ca89-4394-a5a5-c96afc2a44b7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-af303a47-ca89-4394-a5a5-c96afc2a44b7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving syslog to syslog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import array_contains\n",
        "dfLog= spark.read.text(\"syslog\")\n",
        "num_lines = dfLog.count()\n",
        "colvalue = col('value')\n",
        "dfLogbadlines = dfLog.filter(colvalue.contains(\"WARNING\") | colvalue.contains(\"ERROR\") )\n",
        "dfLogbadlines.show(15,truncate=False)\n",
        "print(\"There is {0} bad lines out of  {1} lines  in total \".format(dfLogbadlines.count(),num_lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t85-6Ea1dyR0",
        "outputId": "159e2e6d-35cc-4470-bdc3-08661bcf73f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|value                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Nov 22 08:00:40 aurelien ovpn-cytech.students[1567]: WARNING: 'cipher' is present in local config but missing in remote config, local='cipher BF-CBC'                                                                                                                                                                                                                                                  |\n",
            "|Nov 22 09:01:26 aurelien ovpn-cytech.students[1567]: ERROR: Linux route delete command failed: external program exited with error status: 2                                                                                                                                                                                                                                                            |\n",
            "|Nov 22 09:01:26 aurelien ovpn-cytech.students[1567]: ERROR: Linux route delete command failed: external program exited with error status: 2                                                                                                                                                                                                                                                            |\n",
            "|Nov 22 09:01:26 aurelien ovpn-cytech.students[1567]: ERROR: Linux route delete command failed: external program exited with error status: 2                                                                                                                                                                                                                                                            |\n",
            "|Nov 22 09:01:26 aurelien ovpn-cytech.students[1567]: ERROR: Linux route delete command failed: external program exited with error status: 2                                                                                                                                                                                                                                                            |\n",
            "|Nov 22 09:01:26 aurelien ovpn-cytech.students[1567]: ERROR: Linux route delete command failed: external program exited with error status: 2                                                                                                                                                                                                                                                            |\n",
            "|Nov 22 09:01:37 aurelien gnome-shell[2421]: JS WARNING: [resource:///org/gnome/shell/gdm/util.js 285]: reference to undefined property \"_currentMessageExtraInterval\"                                                                                                                                                                                                                                  |\n",
            "|Nov 22 09:03:02 aurelien ovpn-cytech.students[1567]: WARNING: INSECURE cipher with block size less than 128 bit (64 bit).  This allows attacks like SWEET32.  Mitigate by using a --cipher with a larger block size (e.g. AES-256-CBC).                                                                                                                                                                |\n",
            "|Nov 22 09:03:02 aurelien ovpn-cytech.students[1567]: WARNING: INSECURE cipher with block size less than 128 bit (64 bit).  This allows attacks like SWEET32.  Mitigate by using a --cipher with a larger block size (e.g. AES-256-CBC).                                                                                                                                                                |\n",
            "|Nov 22 09:03:02 aurelien ovpn-cytech.students[1567]: WARNING: cipher with small block size in use, reducing reneg-bytes to 64MB to mitigate SWEET32 attacks.                                                                                                                                                                                                                                           |\n",
            "|Nov 22 09:04:51 aurelien gitlab-runner[1278]: #033[0;33mWARNING: Checking for jobs... failed              #033[0;m  #033[0;33mrunner#033[0;m=o4zz6H2N #033[0;33mstatus#033[0;m=couldn't execute POST against https://gitlab.etude.eisti.fr/api/v4/jobs/request: Post https://gitlab.etude.eisti.fr/api/v4/jobs/request: read tcp 172.21.24.175:51830->194.57.186.19:443: read: connection reset by peer|\n",
            "|Nov 22 09:11:10 aurelien /usr/lib/gdm3/gdm-x-session[2286]: (EE) event6  - DELL09A1:00 0488:121F Touchpad: WARNING: log rate limit exceeded (5 msgs per 7200000ms). Discarding future messages.                                                                                                                                                                                                        |\n",
            "|Nov 22 09:13:50 aurelien org.gnome.Nautilus[5966]: [6004:6004:1122/091350.003485:ERROR:shared_image_manager.cc(189)] SharedImageManager::ProduceSkia: Trying to Produce a Skia representation from a non-existent mailbox.                                                                                                                                                                             |\n",
            "|Nov 22 09:13:50 aurelien org.gnome.Nautilus[5966]: [6004:6004:1122/091350.004291:ERROR:shared_image_manager.cc(189)] SharedImageManager::ProduceSkia: Trying to Produce a Skia representation from a non-existent mailbox.                                                                                                                                                                             |\n",
            "|Nov 22 09:18:10 aurelien gitlab-runner[1278]: #033[31;1mERROR: Runner https://gitlab.com/S17xKbhTxBrBHdezyou9 is not healthy, but will be checked!#033[0;m                                                                                                                                                                                                                                             |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 15 rows\n",
            "\n",
            "There is 402 bad lines out of  12052 lines  in total \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 5.1**"
      ],
      "metadata": {
        "id": "uSByt1Ti3bWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from operator import add\n",
        "\n",
        "\n",
        "rdd = sc.textFile(os.environ[\"DRIVE_DATA\"] + \"/quijote.txt\")\n",
        "words = rdd.flatMap(lambda x: re.findall(r'\\w+', x)).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
        "keys = words.keys()\n",
        "values = words.values()\n",
        "print(\"There is {0} lines in the file .There is  {1} words in total and {2} different words \".format(rdd.count(),values.reduce(add),keys.count()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCpZ7N4X3dxc",
        "outputId": "5e48b4a6-4e7b-4eb5-f618-3a23185614d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is 37861 lines in the file .There is  384442 words in total and 25280 different words \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 5.2**"
      ],
      "metadata": {
        "id": "TUXzF6m7-gr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "people = sc.textFile(os.environ[\"DRIVE_DATA\"] + \"/people.txt\")\n",
        "dict_people = people.map(lambda x:(int(x.split('\\t')[1]),1)).countByKey()\n",
        "X = dict_people.keys()\n",
        "Y = dict_people.values()\n",
        "plt.bar(X,Y,width = 2)\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Number of people')\n",
        "plt.xlim(15,60)\n",
        "plt.ylim(0,3)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "PtILklL29fyv",
        "outputId": "906f6a24-15d1-4f1c-d87e-45ab9f7fd4b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 5.3**"
      ],
      "metadata": {
        "id": "uhngEhtRHyKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patent = sc.textFile(os.environ[\"DRIVE_DATA\"] + \"/cite75_99.txt\").cache()\n",
        "header = patent.first()\n",
        "rdd = patent.filter(lambda line : line!=header) #remove header\n"
      ],
      "metadata": {
        "id": "GmcpScGmJLRg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = rdd.map(lambda x: ( int(x.split(',')[1]),1)).reduceByKey(lambda x, y: x + y).cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cGOZq3hW4mb",
        "outputId": "c5ea4797-4692-41a1-9e90-01c7bdbf2f3d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3557384, 13), (14040, 1), (3755824, 9), (3641592, 9)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "test2 = rdd.map(lambda x:  (int(x.split(',')[0]),1)).cache()\n",
        "cited = np.array(test.keys().collect())\n",
        "citing = np.array(test2.keys().collect())\n",
        "diff = list(set(citing) - set(cited))# those who are citing but  not cited"
      ],
      "metadata": {
        "id": "TfqIQO-wCd8t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructField, StructType, IntegerType\n",
        "columns = [\"PatentNum\",\"ncitations\"]\n",
        "schemaMarks = StructType([\n",
        "    StructField(\"PatentNum\", IntegerType(), True),\n",
        "    StructField(\"ncitations\", IntegerType(), True),\n",
        "    ])\n",
        "data = [(int(patent),0) for patent in diff]\n",
        "added_df = spark.createDataFrame(data, schema = schemaMarks)\n",
        "Dataframe_patent = spark.createDataFrame(test, schema = schemaMarks).union(added_df)"
      ],
      "metadata": {
        "id": "JRUfWlhsFjrj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataframe_patent.show(5,truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c__UGm1DNvFM",
        "outputId": "7489af11-7f26-4172-c7ff-6b10fe20de24"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+\n",
            "|PatentNum|ncitations|\n",
            "+---------+----------+\n",
            "|3557384  |13        |\n",
            "|14040    |1         |\n",
            "|3755824  |9         |\n",
            "|3641592  |9         |\n",
            "|3706104  |5         |\n",
            "+---------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 11**"
      ],
      "metadata": {
        "id": "tXHHi0xksbSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option(\"multiline\",\"true\") \\\n",
        "        .json(os.environ[\"DRIVE_DATA\"] + \"/sw.txt\")\\\n",
        "        .cache()\n",
        "df.show() # read and show the file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_HMFYLDseIS",
        "outputId": "14ca6efe-65ec-4899-9802-f24fdaa8a8e6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|               links|               nodes|\n",
            "+--------------------+--------------------+\n",
            "|[{0, 1, 32}, {2, ...|[{#000000, DARTH ...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(sc)\n",
        "from graphframes import *\n",
        "# Create a GraphFrame\n",
        "nodes, links = df.select('nodes').collect(), df.select('links').collect()\n",
        "data_vertex, data_edges = [], []\n",
        "columns_1, columns_2 = [\"id\",\"value\", \"colour\", \"name\"], [\"src\", \"dst\", \"relationship\"]\n",
        "for i,row in enumerate(nodes[0][0]):\n",
        "  liste = (i,row['value'],row['colour'],row['name'])\n",
        "  data_vertex.append(liste)\n",
        "for row in links[0][0]:\n",
        "  liste = (row['source'],row['target'],row['value'])\n",
        "  data_edges.append(liste)\n",
        "vertex = sqlContext.createDataFrame(data_vertex).toDF(*columns_1)\n",
        "edges = sqlContext.createDataFrame(data_edges).toDF(*columns_2)\n",
        "g = GraphFrame(vertex, edges)\n",
        "vertex.show()\n",
        "edges.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tY23odftJIH",
        "outputId": "d60bfbdf-6bda-4eba-8faf-323034200113"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark/python/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-------+--------------+\n",
            "| id|value| colour|          name|\n",
            "+---+-----+-------+--------------+\n",
            "|  0|  190|#000000|   DARTH VADER|\n",
            "|  1|  171|#bde0f6|         R2-D2|\n",
            "|  2|  145|#A0522D|     CHEWBACCA|\n",
            "|  3|   40|#eb5d00|          BB-8|\n",
            "|  4|   62|#4f4fb1|       QUI-GON|\n",
            "|  5|   25|#808080|   NUTE GUNRAY|\n",
            "|  6|    4|#808080|          PK-4|\n",
            "|  7|    5|#808080|         TC-14|\n",
            "|  8|  148|#48D1CC|       OBI-WAN|\n",
            "|  9|    4|#808080|        DOFINE|\n",
            "| 10|   11|#808080|          RUNE|\n",
            "| 11|    5|#808080|       TEY HOW|\n",
            "| 12|   52|#191970|       EMPEROR|\n",
            "| 13|   20|#808080|CAPTAIN PANAKA|\n",
            "| 14|    9|#808080|    SIO BIBBLE|\n",
            "| 15|   42|#9a9a00|       JAR JAR|\n",
            "| 16|    4|#808080|       TARPALS|\n",
            "| 17|    5|#808080|     BOSS NASS|\n",
            "| 18|   75|#DDA0DD|         PADME|\n",
            "| 19|   12|#808080|      RIC OLIE|\n",
            "+---+-----+-------+--------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+---+---+------------+\n",
            "|src|dst|relationship|\n",
            "+---+---+------------+\n",
            "|  0|  1|          32|\n",
            "|  2|  0|           2|\n",
            "|  0| 20|           5|\n",
            "|  0|  4|          22|\n",
            "|  0| 18|          41|\n",
            "|  0| 21|           2|\n",
            "|  0| 15|          12|\n",
            "|  0| 22|           2|\n",
            "|  0| 23|           8|\n",
            "| 24|  0|          11|\n",
            "|  0| 26|           3|\n",
            "|  0| 27|           2|\n",
            "|  0|  8|          47|\n",
            "|  0| 29|           1|\n",
            "|  0| 30|           1|\n",
            "| 13|  0|           2|\n",
            "|  0| 19|           4|\n",
            "|  0| 32|           9|\n",
            "|  0| 33|           2|\n",
            "|  0| 34|           9|\n",
            "+---+---+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "print(\"There is {0} different characters .\".format(g.vertices.count()))\n",
        "print(\"There is {0} interactions .\".format(g.edges.count()))\n",
        "max_deg = g.degrees.sort(col('degree').desc()).first() # get the the highest degree\n",
        "person = g.vertices.where(g.vertices['id']==max_deg['id']).collect()[0]\n",
        "\n",
        "results = g.pageRank(resetProbability=0.01, maxIter=20) #page algorithm\n",
        "max_rank = results.vertices.select(\"id\", \"pagerank\").first()\n",
        "person_rank = g.vertices.where(g.vertices['id']==max_rank['id']).collect()[0]\n",
        "\n",
        "print(\"The personage with the most interactions is {0}  with  {1} interactions\".format(person['name'],max_deg['degree']))\n",
        "print(\"The personage with the highest rank is {0}: rank = {1}\".format(person_rank['name'],max_rank['pagerank']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDvp_pAUAOi3",
        "outputId": "181b1a2b-5723-4282-f813-d6ecd2059606"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is 111 different characters .\n",
            "There is 444 interactions .\n",
            "The personage with the most interactions is DARTH VADER  with  52 interactions\n",
            "The personage with the highest rank is KITSTER: rank = 0.5541619207953362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 12.1**"
      ],
      "metadata": {
        "id": "NQPjtcIKHid8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /tmp/data/\n",
        "!cp \"$DRIVE_DATA\"apat63_99.txt.tar.bz2 \"$DRIVE_DATA\"cite75_99.txt.tar.bz2 /tmp/data\n",
        "%cd /tmp/data\n",
        "!ls\n",
        "!tar -jxf apat63_99.txt.tar.bz2\n",
        "!tar -jxf cite75_99.txt.tar.bz2\n",
        "!rm /tmp/data/*.tar.bz2\n",
        "!ls"
      ],
      "metadata": {
        "id": "lvFF1WeIHmJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ba069f-62cf-4457-99dd-b7b21b1b39a8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/data\n",
            "apat63_99.txt.tar.bz2  cite75_99.txt.tar.bz2\n",
            "apat63_99.txt  cite75_99.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patent = sc.textFile(\"../tmp/data/cite75_99.txt\").cache()\n",
        "header = patent.first()\n",
        "rdd = patent.filter(lambda line : line!=header) #remove header\n",
        "test = rdd.map(lambda x: ( int(x.split(',')[1]),1)).reduceByKey(lambda x, y: x + y).cache()\n",
        "import numpy as np\n",
        "test2 = rdd.map(lambda x:  (int(x.split(',')[0]),1)).cache()\n",
        "cited = np.array(test.keys().collect())\n",
        "citing = np.array(test2.keys().collect())\n",
        "diff = list(set(citing) - set(cited))# those who are citing but  not cited"
      ],
      "metadata": {
        "id": "NcNNmVBTmNs_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructField, StructType, IntegerType\n",
        "columns = [\"PatentNum\",\"ncitations\"]\n",
        "schemaMarks = StructType([\n",
        "    StructField(\"PatentNum\", IntegerType(), True),\n",
        "    StructField(\"ncitations\", IntegerType(), True),\n",
        "    ])\n",
        "data = [(int(patent),0) for patent in diff]\n",
        "added_df = spark.createDataFrame(data, schema = schemaMarks)\n",
        "Dataframe_patent = spark.createDataFrame(test, schema = schemaMarks).union(added_df)\n",
        "Dataframe_patent.sort(col('ncitations')).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orPb5g2rmQzX",
        "outputId": "f6bfd3eb-5b1b-462e-f9d3-6aeec9bf1d94"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+\n",
            "|PatentNum|ncitations|\n",
            "+---------+----------+\n",
            "|  5810243|         0|\n",
            "|  4761658|         0|\n",
            "|  5810242|         0|\n",
            "|  5810229|         0|\n",
            "|  5810233|         0|\n",
            "|  5810238|         0|\n",
            "|  5810241|         0|\n",
            "|  5810224|         0|\n",
            "|  5810228|         0|\n",
            "|  5810231|         0|\n",
            "|  5810232|         0|\n",
            "|  5810235|         0|\n",
            "|  5810237|         0|\n",
            "|  5810239|         0|\n",
            "|  5810240|         0|\n",
            "|  5810222|         0|\n",
            "|  5810223|         0|\n",
            "|  5810225|         0|\n",
            "|  5810226|         0|\n",
            "|  5810230|         0|\n",
            "+---------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame using Parquet\n",
        "Dataframe_patent.write.format(\"parquet\")\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .option(\"compression\", \"gzip\")\\\n",
        "    .save(\"patent.parquet\")"
      ],
      "metadata": {
        "id": "lF9nGhYSmZza"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apat = spark.read.format(\"csv\")\\\n",
        "                    .option(\"mode\", \"FAILFAST\")\\\n",
        "                    .option(\"sep\", \",\")\\\n",
        "                    .option(\"inferSchema\", \"true\")\\\n",
        "                    .option(\"header\", \"true\")\\\n",
        "                    .load(\"../tmp/data/apat63_99.txt\")\n",
        "columns2 = [\"PatentNum\",\"Country\",\"Year\"]\n",
        "Dataframe_apat = apat.select(expr(\"PATENT AS PatentNum\"),expr(\"COUNTRY AS Country\"),expr(\"GYEAR AS Year\"))\n",
        "Dataframe_apat.show()\n",
        "# Save the DataFrame using Parquet\n",
        "Dataframe_apat.write.format(\"parquet\")\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .option(\"compression\", \"gzip\")\\\n",
        "    .save(\"apat.parquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVtxDnoEmgaC",
        "outputId": "6ab5aafd-e1ec-41f2-f6ac-ae8337a29c1d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+----+\n",
            "|PatentNum|Country|Year|\n",
            "+---------+-------+----+\n",
            "|  3070801|     BE|1963|\n",
            "|  3070802|     US|1963|\n",
            "|  3070803|     US|1963|\n",
            "|  3070804|     US|1963|\n",
            "|  3070805|     US|1963|\n",
            "|  3070806|     US|1963|\n",
            "|  3070807|     US|1963|\n",
            "|  3070808|     US|1963|\n",
            "|  3070809|     US|1963|\n",
            "|  3070810|     US|1963|\n",
            "|  3070811|     US|1963|\n",
            "|  3070812|     US|1963|\n",
            "|  3070813|     US|1963|\n",
            "|  3070814|     US|1963|\n",
            "|  3070815|     US|1963|\n",
            "|  3070816|     US|1963|\n",
            "|  3070817|     US|1963|\n",
            "|  3070818|     US|1963|\n",
            "|  3070819|     US|1963|\n",
            "|  3070820|     GB|1963|\n",
            "+---------+-------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 12.2**"
      ],
      "metadata": {
        "id": "Szt8Mlhxmhkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrieve a DataFrame reading it from the Parquet format\n",
        "apat = spark.read\\\n",
        "            .format(\"parquet\")\\\n",
        "            .option(\"mode\", \"FAILFAST\")\\\n",
        "            .option(\"compression\", \"gzip\")\\\n",
        "            .load(\"apat.parquet\")\n",
        "patent = spark.read\\\n",
        "            .format(\"parquet\")\\\n",
        "            .option(\"mode\", \"FAILFAST\")\\\n",
        "            .option(\"compression\", \"gzip\")\\\n",
        "            .load(\"patent.parquet\")\n",
        "apat.cache()\n",
        "patent.cache()\n",
        "apat = apat.withColumnRenamed(\"Country\", \"code\")\n",
        "#country codes\n",
        "countries = spark.read.format(\"csv\")\\\n",
        "                    .option(\"sep\", \"\\t\")\\\n",
        "                    .option(\"inferSchema\", \"true\")\\\n",
        "                    .option(\"header\", \"false\")\\\n",
        "                    .load(os.environ[\"DRIVE_DATA\"] + \"/country_codes.txt\")\n",
        "countries = countries.withColumnRenamed(\"_c0\", \"code\")\n",
        "countries.show() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZCyYM4ymmPX",
        "outputId": "ee0fc737-c233-49ea-f782-02afceb10961"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------------+\n",
            "|code|                _c1|\n",
            "+----+-------------------+\n",
            "|  AF|        Afghanistan|\n",
            "|  AX|      Aland Islands|\n",
            "|  AL|            Albania|\n",
            "|  DZ|            Algeria|\n",
            "|  AS|     American Samoa|\n",
            "|  AD|            Andorra|\n",
            "|  AO|             Angola|\n",
            "|  AI|           Anguilla|\n",
            "|  AQ|         Antarctica|\n",
            "|  AG|Antigua and Barbuda|\n",
            "|  AR|          Argentina|\n",
            "|  AM|            Armenia|\n",
            "|  AW|              Aruba|\n",
            "|  AC|   Ascension Island|\n",
            "|  AU|          Australia|\n",
            "|  AT|            Austria|\n",
            "|  AZ|         Azerbaijan|\n",
            "|  BS|            Bahamas|\n",
            "|  BH|            Bahrain|\n",
            "|  BD|         Bangladesh|\n",
            "+----+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum,avg,max,count\n",
        "result = patent.join(apat,\"PatentNum\")\n",
        "result = result.filter(col(\"ncitations\")>0) #only those who are cited can count in the computation of the mean\n",
        "resultgrouped = result.groupBy('Code','Year')\\\n",
        ".agg( count(\"PatentNum\").alias(\"Patents\"),\\\n",
        "      sum(\"ncitations\").alias(\"TotalCitations\"),\\\n",
        "      avg(\"ncitations\").alias(\"AverageCitations\"),\\\n",
        "      max(\"ncitations\").alias(\"MaxCitations\")\\\n",
        "      )\n",
        "resultgrouped = countries.join(resultgrouped,\"Code\").withColumnRenamed(\"_c1\", \"Country\").drop(\"Code\").sort(col(\"Country\"),col(\"Year\")) #join on contry and sort \n",
        "resultgrouped.show(20)\n",
        "resultgrouped.write.option(\"header\",True).format(\"CSV\").mode(\"overwrite\").save(\"patents_groupedby_countries.csv\") #Save on csv format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoxX2i6_msSK",
        "outputId": "376bbc3f-9e52-4375-da58-d522dcb4651f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+-------+--------------+-----------------+------------+\n",
            "|            Country|Year|Patents|TotalCitations| AverageCitations|MaxCitations|\n",
            "+-------------------+----+-------+--------------+-----------------+------------+\n",
            "|            Algeria|1963|      2|             7|              3.5|           4|\n",
            "|            Algeria|1968|      1|             2|              2.0|           2|\n",
            "|            Algeria|1970|      1|             2|              2.0|           2|\n",
            "|            Algeria|1972|      1|             1|              1.0|           1|\n",
            "|            Algeria|1977|      1|             2|              2.0|           2|\n",
            "|            Andorra|1987|      1|             3|              3.0|           3|\n",
            "|            Andorra|1993|      1|             1|              1.0|           1|\n",
            "|            Andorra|1998|      1|             1|              1.0|           1|\n",
            "|Antigua and Barbuda|1978|      1|             6|              6.0|           6|\n",
            "|Antigua and Barbuda|1979|      1|            14|             14.0|          14|\n",
            "|Antigua and Barbuda|1991|      1|             8|              8.0|           8|\n",
            "|Antigua and Barbuda|1994|      1|            19|             19.0|          19|\n",
            "|Antigua and Barbuda|1995|      2|            12|              6.0|          11|\n",
            "|Antigua and Barbuda|1996|      2|             3|              1.5|           2|\n",
            "|          Argentina|1963|     14|            35|              2.5|           7|\n",
            "|          Argentina|1964|     20|            60|              3.0|           8|\n",
            "|          Argentina|1965|     10|            35|              3.5|          10|\n",
            "|          Argentina|1966|     16|            44|             2.75|           9|\n",
            "|          Argentina|1967|     13|            60|4.615384615384615|          14|\n",
            "|          Argentina|1968|     14|            80|5.714285714285714|          21|\n",
            "+-------------------+----+-------+--------------+-----------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 12.3**"
      ],
      "metadata": {
        "id": "HAVOs-RSnNd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apat = spark.read.format(\"csv\")\\\n",
        "                    .option(\"mode\", \"FAILFAST\")\\\n",
        "                    .option(\"sep\", \",\")\\\n",
        "                    .option(\"inferSchema\", \"true\")\\\n",
        "                    .option(\"header\", \"true\")\\\n",
        "                    .option(\"nullValue\", \"null\")\\\n",
        "                    .option(\"compression\", \"bzip2\")\\\n",
        "                    .load(os.environ[\"DRIVE_DATA\"] +\"apat63_99.txt.tar.bz2\")\\\n",
        "                    .rdd\\\n",
        "                    .repartition(8) #8 partitions\n",
        "apat.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRpspCyXm-yr",
        "outputId": "3a26e8de-a107-4784-e5eb-d496e7722646"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapPartitionsRDD[651] at coalesce at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def spliting(Partition):\n",
        "  for row in Partition:\n",
        "    yield((row.COUNTRY , int(row.GYEAR)),1) # each (country,year) is a key with value 1\n",
        "rdd_apat = apat.mapPartitions(spliting)\n",
        "rdd_apat.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLT_BI0-22hz",
        "outputId": "84c9195f-c9af-441d-e636-e8a182db9a10"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[652] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def regroup(x,y):\n",
        "  z = []\n",
        "  if type(x) is list:\n",
        "    for element in x:\n",
        "      z.append(element)\n",
        "  else:\n",
        "    z.append(x)\n",
        "  if type(y) is list:\n",
        "    for element in y:\n",
        "      z.append(element)\n",
        "  else:\n",
        "    z.append(y)  \n",
        "  return sorted(z)\n",
        "\n",
        "tmp1 = rdd_apat.reduceByKey(lambda x,y:x+y).cache()\n",
        "tmp2 = tmp1.map(lambda x: (x[0][0],(x[0][1],x[1]))).cache()\n",
        "groups =  tmp2.reduceByKey(regroup).sortByKey() #sort by country\n",
        "groups.lookup('PA')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRpUogV423T9",
        "outputId": "f35426f8-623c-4f3e-d49f-934823a7f95f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(1963, 2),\n",
              "  (1964, 2),\n",
              "  (1965, 1),\n",
              "  (1966, 1),\n",
              "  (1970, 1),\n",
              "  (1971, 1),\n",
              "  (1972, 6),\n",
              "  (1974, 3),\n",
              "  (1975, 5),\n",
              "  (1976, 3),\n",
              "  (1977, 2),\n",
              "  (1978, 2),\n",
              "  (1980, 2),\n",
              "  (1982, 1),\n",
              "  (1983, 1),\n",
              "  (1985, 2),\n",
              "  (1986, 1),\n",
              "  (1987, 2),\n",
              "  (1988, 1),\n",
              "  (1990, 1),\n",
              "  (1991, 2),\n",
              "  (1993, 1),\n",
              "  (1995, 1),\n",
              "  (1996, 1),\n",
              "  (1999, 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 12.4**"
      ],
      "metadata": {
        "id": "QpQm2r8O267G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apat = spark.read\\\n",
        "            .format(\"parquet\")\\\n",
        "            .option(\"mode\", \"FAILFAST\")\\\n",
        "            .option(\"compression\", \"gzip\")\\\n",
        "            .load(\"apat.parquet\")\n",
        "patent = spark.read\\\n",
        "            .format(\"parquet\")\\\n",
        "            .option(\"mode\", \"FAILFAST\")\\\n",
        "            .option(\"compression\", \"gzip\")\\\n",
        "            .load(\"patent.parquet\")\n",
        "apat.cache()\n",
        "patent.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekQilQdn2-ep",
        "outputId": "16c7374f-88e8-4934-b83c-d31a23a8e723"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[PatentNum: int, ncitations: int]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum,avg,max,count\n",
        "from pyspark.sql.functions import col, expr, row_number\n",
        "\n",
        "result = patent.join(apat,\"PatentNum\")\n",
        "result = result.filter(col(\"ncitations\")>0)\n",
        "resultgrouped = result.groupBy('Country','Year')\\\n",
        ".agg( sum(\"ncitations\").alias(\"TotalCitations\"),\\\n",
        "      avg(\"ncitations\").alias(\"AverageCitations\"),\\\n",
        "      max(\"ncitations\").alias(\"MaxCitations\")\\\n",
        "      )\n",
        "from pyspark.sql.window import Window\n",
        "resultgrouped  = resultgrouped.withColumn(\"Diff\", col(\"MaxCitations\")-col(\"AverageCitations\")).sort(col(\"Country\"),col(\"Year\"))\n",
        "resultgrouped = resultgrouped.join(result,[\"Country\",\"Year\"],\"inner\")\n",
        "# Specify the windows to partition the rows by the userId column\n",
        "window = Window.partitionBy(col(\"Year\"),col(\"Country\")).orderBy(col(\"TotalCitations\").desc())\n",
        "# Create a column with the maximum score per user\n",
        "colMax = max(col(\"TotalCitations\")).over(window)\n",
        "resultgrouped  = resultgrouped.withColumn(\"row\",row_number().over(window)).filter(col(\"row\") ==1).drop(\"row\")\n",
        "resultgrouped = resultgrouped.sort(col(\"Country\"),col(\"Year\")).drop(\"TotalCitations\",\"ncitations\")\n",
        "resultgrouped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mK77L2x3Bh5",
        "outputId": "513d374b-c171-4f57-c57b-219500f38e99"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+----------------+------------+----+---------+\n",
            "|Country|Year|AverageCitations|MaxCitations|Diff|PatentNum|\n",
            "+-------+----+----------------+------------+----+---------+\n",
            "|     AD|1987|             3.0|           3| 0.0|  4688621|\n",
            "|     AD|1993|             1.0|           1| 0.0|  5193231|\n",
            "|     AD|1998|             1.0|           1| 0.0|  5765303|\n",
            "|     AE|1984|             5.0|           5| 0.0|  4482959|\n",
            "|     AE|1985|            14.0|          14| 0.0|  4554981|\n",
            "|     AE|1987|             3.0|           3| 0.0|  4663181|\n",
            "|     AE|1989|             5.0|           7| 2.0|  4805221|\n",
            "|     AE|1990|             2.0|           2| 0.0|  4909321|\n",
            "|     AE|1991|             2.0|           3| 1.0|  4997041|\n",
            "|     AE|1992|             4.0|           4| 0.0|  5104556|\n",
            "|     AE|1993|             8.0|           8| 0.0|  5181569|\n",
            "|     AE|1996|             1.0|           1| 0.0|  5580125|\n",
            "|     AG|1978|             6.0|           6| 0.0|  4126850|\n",
            "|     AG|1979|            14.0|          14| 0.0|  4172981|\n",
            "|     AG|1991|             8.0|           8| 0.0|  5013035|\n",
            "|     AG|1994|            19.0|          19| 0.0|  5345071|\n",
            "|     AG|1995|             6.0|          11| 5.0|  5457307|\n",
            "|     AG|1996|             1.5|           2| 0.5|  5525786|\n",
            "|     AM|1995|             2.0|           2| 0.0|  5382341|\n",
            "|     AN|1979|             1.0|           1| 0.0|  4165701|\n",
            "+-------+----+----------------+------------+----+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultgrouped.write.option(\"header\",True).format(\"CSV\").mode(\"overwrite\").save(\"maxcitations_grouped_countries.csv\")\n",
        "#save"
      ],
      "metadata": {
        "id": "tmNGyjFf3EV8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 12.5**"
      ],
      "metadata": {
        "id": "4WAesL2p3J2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apat = spark.read\\\n",
        "            .format(\"parquet\")\\\n",
        "            .option(\"mode\", \"FAILFAST\")\\\n",
        "            .option(\"compression\", \"gzip\")\\\n",
        "            .load(\"apat.parquet\")\n",
        "apat = apat.withColumn(\"Decade\",expr(\"Year - Year % 10\")).cache() #get the decade\n",
        "def difference(df): #difference between two year for a same country\n",
        "  v = df.Diff\n",
        "  u = df.Patents\n",
        "  for i,element in enumerate(u.values):\n",
        "    if i > 0:\n",
        "      v.iloc[i] = element-u.iloc[i-1]\n",
        "  v.iloc[0] = 0\n",
        "  return df.assign(Diff = v)\n",
        "\n",
        "apat = apat.groupBy('Country','Decade')\\\n",
        "            .agg( count(\"PatentNum\").alias(\"Patents\"))\\\n",
        "            .withColumn(\"Diff\",expr(\"Patents\"))\\\n",
        "            .sort(\"Country\",\"Decade\")\n",
        "apat = apat.groupBy('Country').applyInPandas(difference,schema =\"Country string, Decade int, Patents int,Diff int\")\\\n",
        "            \n",
        "apat.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1FITd4i3NYT",
        "outputId": "778b4fc3-561c-44d9-dad2-aca293478dc1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-------+----+\n",
            "|Country|Decade|Patents|Diff|\n",
            "+-------+------+-------+----+\n",
            "|     AD|  1980|      1|   0|\n",
            "|     AD|  1990|      5|   4|\n",
            "|     AE|  1980|      7|   0|\n",
            "|     AE|  1990|     11|   4|\n",
            "|     AG|  1970|      2|   0|\n",
            "|     AG|  1990|      7|   5|\n",
            "|     AI|  1990|      1|   0|\n",
            "|     AL|  1990|      1|   0|\n",
            "|     AM|  1990|      2|   0|\n",
            "|     AN|  1970|      1|   0|\n",
            "|     AN|  1980|      2|   1|\n",
            "|     AN|  1990|      5|   3|\n",
            "|     AR|  1960|    135|   0|\n",
            "|     AR|  1970|    239| 104|\n",
            "|     AR|  1980|    184| -55|\n",
            "|     AR|  1990|    292| 108|\n",
            "|     AT|  1960|    950|   0|\n",
            "|     AT|  1970|   2588|1638|\n",
            "|     AT|  1980|   3057| 469|\n",
            "|     AT|  1990|   3665| 608|\n",
            "+-------+------+-------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apat.write.option(\"header\",True).format(\"CSV\").mode(\"overwrite\").save(\"exercise12.5.csv\")\n",
        "#save"
      ],
      "metadata": {
        "id": "vEmLOscv3VEi"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}